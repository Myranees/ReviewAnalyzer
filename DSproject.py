# -*- coding: utf-8 -*-
"""DS Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ClisKYzSHG8-V5f99idz-a-hKGuOp0Qe
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install nltk

# Import necessary libraries
import pandas as pd
import numpy as np
import re
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import confusion_matrix

# Download necessary NLTK data
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

data = pd.read_csv('/content/drive/MyDrive/SEM 5 ANIS/DS/Womens Clothing E-Commerce Reviews.csv')

"""# Data Exploration"""

data.head()

data.shape

data.info()

data.isnull().sum()

data.duplicated().sum()

"""# Data Cleaning"""

# Drop irrelevant columns
data = data.drop(columns='Unnamed: 0')

# change the name of columns to be easy to use
c={'Clothing ID':'Clothing_ID','Review Text':'Review_Text','Recommended IND':'Recommended_IND',
   'Positive Feedback Count':'P_Feedback_Count','Division Name':'Division_Name','Department Name':'Department_Name',
   'Class Name':'Class_Name'}

data.rename(columns=c,inplace=True)

# Drop rows with missing review text
data = data.dropna(subset=['Review_Text'])

# drop (nan) in columns that not effect in row of the data
data.dropna(subset=['Division_Name',
                   'Department_Name', 'Class_Name'],inplace=True)

# Fill missing values in the 'Title' column with empty strings
data['Title'] = data['Title'].fillna('')

data.isnull().sum()

def clean_text(text):
    # Convert to lowercase
    text = text.lower()
    # Retain alphanumeric characters and essential punctuation
    text = re.sub(r'[^a-zA-Z0-9\s.,]', '', text)
    # Tokenize sentences and words
    sentences = nltk.sent_tokenize(text)
    words = [word_tokenize(sentence) for sentence in sentences]
    # Flatten the list of words
    words = [word for sublist in words for word in sublist]
    # Remove custom stopwords
    stop_words = set(stopwords.words('english')) - {'not', 'but', 'very'}
    words = [word for word in words if word not in stop_words]
    # Lemmatize words
    lemmatizer = WordNetLemmatizer()
    words = [lemmatizer.lemmatize(word) for word in words]
    # Rejoin words back into cleaned text
    return ' '.join(words)

!pip install nltk
import nltk
import re
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download the 'punkt_tab' resource
nltk.download('punkt_tab')

# Apply cleaning to 'Review Text' and 'Title' columns
data['Cleaned_ReviewText'] = data['Review_Text'].apply(clean_text)
data['Cleaned_Title'] = data['Title'].apply(clean_text)

# Display the cleaned data
print(data[['Cleaned_Title', 'Cleaned_ReviewText']].head())

print("Original Text:", data['Review_Text'][0])
print("Cleaned Text:", data['Cleaned_ReviewText'][0])

sentences = nltk.sent_tokenize(data['Review_Text'][0])
print(sentences)

"""# EDA"""

# Distribution of Ratings
import matplotlib.pyplot as plt
import seaborn as sns

print(data.describe())

print(data[['Division_Name', 'Department_Name', 'Class_Name']].nunique())

#check the distribution of ratings
plt.figure(figsize=(8, 5))
sns.countplot(x='Rating', data=data, palette='viridis')
plt.title('Distribution of Ratings')
plt.xlabel('Rating')
plt.ylabel('Count')
plt.show()

#check recommendation distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='Recommended_IND', data=data, palette='coolwarm')
plt.title('Distribution of Recommendations')
plt.xlabel('Recommended (1 = Yes, 0 = No)')
plt.ylabel('Count')
plt.show()

#check positive feedback count
plt.figure(figsize=(10, 6))
sns.histplot(data['P_Feedback_Count'], bins=30, kde=True, color='skyblue')
plt.title('Distribution of Positive Feedback Count')
plt.xlabel('Positive Feedback Count')
plt.ylabel('Frequency')
plt.show()

data['Review_Length'] = data['Cleaned_ReviewText'].apply(lambda x: len(x.split()))
plt.figure(figsize=(10, 6))
sns.histplot(data['Review_Length'], bins=20, kde=True, color='coral')
plt.title('Distribution of Review Length')
plt.xlabel('Number of Words in Review')
plt.ylabel('Frequency')
plt.show()

#make visualize for age to see our target consumer
plt.figure(figsize=(11,5))
sns.histplot(x='Age',data=data,bins=40)
plt.xticks(np.arange(20,90,5))
plt.show()

data['age_group'] = pd.cut(
    x=data['Age'],
    bins=[10, 20, 30, 40, 50, 60, 70, 80, 90, 100],
    labels=['10-19','20-29', '30-39', '40-49', '50-59', '60-69', '70-79','80-89','90-99']
)

plt.figure(figsize=(8,5))
ax=sns.countplot(x=data['age_group'], palette='pastel')

plt.title('sales for each age group')
for i in ax.patches:
    ax.text(i.get_x()+.06,i.get_height()+2.6,str(int((i.get_height()))),
            rotation=0,fontsize=12,color='black')
plt.show()

data['age_group'] = data['age_group'].astype(str)

"""Group 30 until 39 has the most highest sales"""

# Basic statistics for numerical features
print(data[['Rating', 'Recommended_IND', 'P_Feedback_Count']].describe())

# Box plot of Positive Feedback Count by Rating
plt.figure(figsize=(10, 6))
sns.boxplot(x='Rating', y='P_Feedback_Count', data=data)
plt.title('Positive Feedback Count by Rating')
plt.xlabel('Rating')
plt.ylabel('Positive Feedback Count')
plt.show()

# Distribution of Division, Department, and Class Names
plt.figure(figsize=(14, 5))
sns.countplot(x='Division_Name', data=data)
plt.title('Distribution of Division Names')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(14, 5))
sns.countplot(x='Department_Name', data=data)
plt.title('Distribution of Department Names')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(14, 5))
sns.countplot(x='Class_Name', data=data)
plt.title('Distribution of Class Names')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(10,8),dpi=60)
ax=sns.countplot(x='Department_Name',data=data,hue='age_group')
plt.title('Department Name & age')

"""Tops department has the highest count within the age group 30-39"""

recommended = data[data['Recommended_IND']==1]
not_recommended = data[data['Recommended_IND']==0]
ax = sns.countplot(recommended['Department_Name'], color="blue", alpha = 0.8, label = "Recommended")
ax = sns.countplot(not_recommended['Department_Name'], color="red", alpha = 0.8, label = "Not Recommended")
ax = plt.title("Recommended Items in each Department")
ax = plt.legend()
plt.show()

plt.figure(figsize=(8, 6))
sns.boxplot(x='Recommended_IND', y='Rating', data=data, palette='magma')
plt.title('Ratings by Recommendation Indicator')
plt.xlabel('Recommended (1 = Yes, 0 = No)')
plt.ylabel('Rating')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(x='Rating', y='Review_Length', data=data, palette='pastel')
plt.title('Review Length by Rating')
plt.xlabel('Rating')
plt.ylabel('Review Length (Word Count)')
plt.show()

# Select only numerical columns for correlation
numeric_data = data.select_dtypes(include=[np.number])

# Compute the correlation matrix
corr = numeric_data.corr()

# Plot the heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f', cbar=True, square=True)
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

"""Rating has positive correlation with the Recommended_IND

# Data Labeling

Sentiment Analysis Preparation using TextBlob to categorize reviews as positive, negative or constructive based on their content
"""

data['Polarity'] = data['Cleaned_ReviewText'].apply(lambda x: TextBlob(x).sentiment.polarity)

# Visualize the polarity distribution
import matplotlib.pyplot as plt
plt.figure(figsize=(10, 6))
plt.hist(data['Polarity'], bins=50, color='skyblue', edgecolor='black')
plt.title('Polarity Score Distribution')
plt.xlabel('Polarity')
plt.ylabel('Frequency')
plt.show()

# Function to classify sentiment
def get_sentiment(text):
    score = TextBlob(text).sentiment.polarity
    if score > 0.25:
        return 'Positive'
    elif score < 0.1:
        return 'Negative'
    else:
        return 'Constructive'

# Apply sentiment function
data['Sentiment'] = data['Cleaned_ReviewText'].apply(get_sentiment)

# Count the number of rows for each sentiment
sentiment_counts = data['Sentiment'].value_counts()

# Print the counts for each sentiment
for sentiment, count in sentiment_counts.items():
    print(f"{sentiment}: {count}")

# Create the pie chart
plt.figure(figsize=(8, 8))
sentiment_counts.plot.pie(
    autopct='%1.1f%%',  # Display percentage
    startangle=90,      # Rotate the pie chart
    colors=['skyblue', 'lightgreen', 'salmon'],  # Colors for the slices
    labels=sentiment_counts.index  # Labels for each sentiment
)

# Add a title
plt.title('Sentiment Distribution')
plt.ylabel('')  # Hide the y-axis label
plt.show()

# Display the first 5 rows of the DataFrame with 'Review_Text' and 'Sentiment' side by side
data[['Review_Text', 'Sentiment']].head()

# Display the first 20 positive reviews with their corresponding sentiment
count = 0
for index, row in data.iterrows():
    if count >= 20:
        break
    if row['Sentiment'] == 'Positive':  # Only show positive reviews
        print(f"Review: {row['Review_Text']}")
        print(f"Sentiment: {row['Sentiment']}")
        print("-" * 20)
        count += 1

# Display the first 20 negative reviews with their corresponding sentiment
count = 0
for index, row in data.iterrows():
    if count >= 20:
        break
    if row['Sentiment'] == 'Negative':  # Only show negative reviews
        print(f"Review: {row['Review_Text']}")
        print(f"Sentiment: {row['Sentiment']}")
        print("-" * 20)
        count += 1

# Display the first 20 constructive reviews with their corresponding sentiment
count = 0
for index, row in data.iterrows():
    if count >= 20:
        break
    if row['Sentiment'] == 'Constructive':  # Only show constructive reviews
        print(f"Review: {row['Review_Text']}")
        print(f"Sentiment: {row['Sentiment']}")
        print("-" * 20)
        count += 1

"""After labeling the data, sentiment classification is enhanced by using ML model

# Objective 1: To summarize customer reviews using extractive text mining techniques
"""

# Word frequency analysis in all reviews
from sklearn.feature_extraction.text import CountVectorizer
vectorizer = CountVectorizer(stop_words='english', max_features=20)
common_words = vectorizer.fit_transform(data['Cleaned_ReviewText'])
sum_words = common_words.sum(axis=0)
words_freq = [(word, sum_words[0, idx]) for word, idx in vectorizer.vocabulary_.items()]
words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)

# Plot common words
plt.figure(figsize=(10, 6))
sns.barplot(x=[w[0] for w in words_freq], y=[w[1] for w in words_freq])
plt.title('Top 20 Most Common Words in Reviews')
plt.xlabel('Words')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
import numpy as np

# Join all cleaned reviews into a single string
all_reviews = ' '.join(data['Cleaned_ReviewText'])

# Define a custom color function
def custom_color_func(word, font_size, position, orientation, random_state=None, **kwargs):
    if font_size < 30:  # Lesser frequent words (small font sizes)
        return "blue"
    elif 30 <= font_size < 60:  # Moderately frequent words
        return "darkgoldenrod"
    else:  # More frequent words (large font sizes)
        return "tomato"

# Generate Word Cloud with custom colors based on frequency
wordcloud_gradient = WordCloud(
    width=800,
    height=400,
    max_words=50,
    color_func=custom_color_func,
    background_color='white'
).generate(all_reviews)

# Plot the Word Cloud
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud_gradient, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud with Gradient Colors')
plt.show()

# Separate the data into two corpora
recommend_yes = data[data['Recommended_IND'] == 1]['Cleaned_ReviewText']
recommend_no = data[data['Recommended_IND'] == 0]['Cleaned_ReviewText']

from collections import Counter
import seaborn as sns
import matplotlib.pyplot as plt
import re

# Function to clean text by removing punctuation and tokenizing
def preprocess_text(text):
    # Remove punctuation using regex
    text = re.sub(r'[^\w\s]', '', text)  # Retain only alphanumeric characters and spaces
    text = text.lower()  # Convert to lowercase for consistency
    return text

# Apply cleaning function to the corpora
recommend_yes_clean = recommend_yes.apply(preprocess_text)
recommend_no_clean = recommend_no.apply(preprocess_text)

# Join cleaned reviews into single strings for both corpora
corpus_yes = ' '.join(recommend_yes_clean)
corpus_no = ' '.join(recommend_no_clean)

# Create frequency distributions for each corpus
freq_yes = Counter(corpus_yes.split())  # Split into words after cleaning
freq_no = Counter(corpus_no.split())

# Extract the top 20 most common words for each corpus
top_words_yes = freq_yes.most_common(20)
top_words_no = freq_no.most_common(20)

# Separate words and their frequencies for plotting
words_yes, counts_yes = zip(*top_words_yes)
words_no, counts_no = zip(*top_words_no)

# Plot top 20 words for recommended reviews
plt.figure(figsize=(12, 6))
sns.barplot(x=list(counts_yes), y=list(words_yes), palette='Greens')
plt.title('Top 20 Words in Recommended Reviews')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.show()

# Plot top 20 words for not recommended reviews
plt.figure(figsize=(12, 6))
sns.barplot(x=list(counts_no), y=list(words_no), palette='Reds')
plt.title('Top 20 Words in Not Recommended Reviews')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.show()

from wordcloud import WordCloud
import matplotlib.pyplot as plt
from collections import Counter

# Combine both corpora into one string
combined_corpus = corpus_yes + ' ' + corpus_no

# Create frequency distributions for both corpora
freq_yes = Counter(corpus_yes.split())
freq_no = Counter(corpus_no.split())

# Find common words (intersection)
common_words = set(freq_yes.keys()).intersection(set(freq_no.keys()))

# Create a corpus for common words by merging words that appear in both corpora
common_corpus = ' '.join([word for word in combined_corpus.split() if word in common_words])

# Find unique words for "yes" and "no" corpora (differences)
unique_yes_words = set(freq_yes.keys()) - set(freq_no.keys())
unique_no_words = set(freq_no.keys()) - set(freq_yes.keys())

# Create corpora for unique words
unique_yes_corpus = ' '.join([word for word in corpus_yes.split() if word in unique_yes_words])
unique_no_corpus = ' '.join([word for word in corpus_no.split() if word in unique_no_words])

# Create Word Cloud for Common Words (Shared words between "yes" and "no")
common_wordcloud = WordCloud(width=800, height=400, background_color='white',
                              colormap='viridis', max_words=100).generate(common_corpus)

# Create Word Cloud for Unique Words in "Yes" Reviews (Recommended)
unique_yes_wordcloud = WordCloud(width=800, height=400, background_color='white',
                                  colormap='Greens', max_words=100).generate(unique_yes_corpus)

# Create Word Cloud for Unique Words in "No" Reviews (Not Recommended)
unique_no_wordcloud = WordCloud(width=800, height=400, background_color='white',
                                 colormap='Reds', max_words=100).generate(unique_no_corpus)

# Plot the word clouds for comparison
plt.figure(figsize=(15, 10))

# Plot the Commonality Cloud
plt.subplot(2, 2, 1)
plt.imshow(common_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Commonality Cloud (Shared Words)')

# Plot the "Yes" Unique Word Cloud (green)
plt.subplot(2, 2, 2)
plt.imshow(unique_yes_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Unique Words in Recommended Reviews (Yes)')

# Plot the "No" Unique Word Cloud (red)
plt.subplot(2, 2, 3)
plt.imshow(unique_no_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Unique Words in Not Recommended Reviews (No)')

plt.tight_layout()
plt.show()

"""Text mining techniques"""

# Check text lengths
data['Text_Length'] = data['Cleaned_ReviewText'].apply(len)
print(data['Text_Length'].describe())

# Check reviews with short lengths
short_reviews = data[data['Text_Length'] <= 50]  # Adjust threshold as needed
print(short_reviews[['Cleaned_ReviewText']].head(10))

pip install lexrank

!pip install rouge-score

from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from collections import Counter
from lexrank import LexRank
from lexrank.mappings.stopwords import STOPWORDS
from rouge_score import rouge_scorer
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
import string

"""TF-IDF"""

def tfidf_summarize_fixed(text, num_sentences=3):
    sentences = sent_tokenize(text)
    if len(sentences) <= num_sentences:
        return text

    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)
    sim_matrix = cosine_similarity(tfidf_matrix)
    ranked_sentences = sorted(((sim_matrix[i].sum(), s) for i, s in enumerate(sentences)), reverse=True)

    summary = ' '.join([ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))])
    return summary

data['TFIDF_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: tfidf_summarize_fixed(x))

"""LexRank"""

def lexrank_summarize_fixed(text, num_sentences=3):
    sentences = sent_tokenize(text)
    if len(sentences) <= num_sentences:
        return text  # Return the original text if too short

    try:
        lexrank = LexRank(sentences, stopwords=STOPWORDS['en'])
        summary = lexrank.get_summary(sentences, summary_size=num_sentences, threshold=0.2)
        return ' '.join(summary)
    except ValueError:  # Catch non-informative document errors
        return ' '.join(sentences[:num_sentences])  # Fallback to the first few sentences


data['LexRank_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: lexrank_summarize_fixed(x))

"""Luhn"""

def luhn_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    if len(sentences) <= num_sentences:
        return text

    words = word_tokenize(text.lower())
    stop_words = set(stopwords.words('english') + list(string.punctuation))
    filtered_words = [word for word in words if word.isalnum() and word not in stop_words]

    word_freq = Counter(filtered_words)
    max_freq = max(word_freq.values())
    word_freq = {word: freq / max_freq for word, freq in word_freq.items()}

    sentence_scores = []
    for sentence in sentences:
        sentence_words = word_tokenize(sentence.lower())
        sentence_score = sum(word_freq.get(word, 0) for word in sentence_words)
        sentence_scores.append((sentence, sentence_score))

    ranked_sentences = sorted(sentence_scores, key=lambda x: x[1], reverse=True)
    summary = ' '.join([ranked_sentences[i][0] for i in range(min(num_sentences, len(ranked_sentences)))])
    return summary
# Apply the fixed Luhn summarization
data['Luhn_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: luhn_summarize(x, num_sentences=3))

"""LSA"""

from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import CountVectorizer

# LSA Summarization
def lsa_summarize(text, num_sentences=3):
    sentences = sent_tokenize(text)
    if len(sentences) <= num_sentences:
        return text

    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)

    svd = TruncatedSVD(n_components=min(num_sentences, tfidf_matrix.shape[1] - 1))
    svd_matrix = svd.fit_transform(tfidf_matrix)

    sentence_scores = svd_matrix.sum(axis=1)
    ranked_sentences = sorted(((score, sentence) for sentence, score in zip(sentences, sentence_scores)), reverse=True)

    summary = ' '.join([ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))])
    return summary

data['LSA_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: lsa_summarize(x))

# Display summaries for comparison
print(data[['Cleaned_ReviewText', 'TFIDF_Summary','LexRank_Summary','Luhn_Summary','LSA_Summary']].head())

# Filter out rows with very short cleaned text
data = data[data['Cleaned_ReviewText'].apply(len) > 10]

# Check a few summaries
for index, row in data[['Cleaned_ReviewText', 'TFIDF_Summary', 'Luhn_Summary', 'LexRank_Summary','LSA_Summary']].head(5).iterrows():
    print(f"Original: {row['Cleaned_ReviewText']}")
    print(f"TFIDF Summary: {row['TFIDF_Summary']}")
    print(f"Luhn Summary: {row['Luhn_Summary']}")
    print(f"LexRank Summary: {row['LexRank_Summary']}")
    print(f"LSA Summary: {row['LSA_Summary']}")
    print("-" * 50)

# Initialize ROUGE scorer and smoothing function for BLEU
rouge_scorer_instance = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)
smooth = SmoothingFunction().method1

def evaluate_summaries(original, summary):
    rouge = rouge_scorer_instance.score(original, summary)
    bleu = sentence_bleu([original.split()], summary.split(), smoothing_function=smooth)
    return rouge['rouge1'].fmeasure, bleu

# Apply improvements
# Assume data is a pandas DataFrame with a column 'Cleaned_ReviewText'
data['TFIDF_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: tfidf_summarize_fixed(x))
data['Luhn_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: luhn_summarize(x))
data['LexRank_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: lexrank_summarize_fixed(x))
data['LSA_Summary'] = data['Cleaned_ReviewText'].apply(lambda x: lsa_summarize(x))

# Recalculate evaluation metrics
results = data.apply(lambda row: evaluate_summaries(row['Cleaned_ReviewText'], row['TFIDF_Summary']), axis=1)
data['TFIDF_ROUGE'], data['TFIDF_BLEU'] = zip(*results)

results = data.apply(lambda row: evaluate_summaries(row['Cleaned_ReviewText'], row['Luhn_Summary']), axis=1)
data['Luhn_ROUGE'], data['Luhn_BLEU'] = zip(*results)

results = data.apply(lambda row: evaluate_summaries(row['Cleaned_ReviewText'], row['LexRank_Summary']), axis=1)
data['LexRank_ROUGE'], data['LexRank_BLEU'] = zip(*results)

results = data.apply(lambda row: evaluate_summaries(row['Cleaned_ReviewText'], row['LSA_Summary']), axis=1)
data['LSA_ROUGE'], data['LSA_BLEU'] = zip(*results)

# Output average scores for all methods
average_rouge = {
    'TFIDF': data['TFIDF_ROUGE'].mean(),
    'Luhn': data['Luhn_ROUGE'].mean(),
    'LexRank': data['LexRank_ROUGE'].mean(),
    'LSA': data['LSA_ROUGE'].mean()
}

average_bleu = {
    'TFIDF': data['TFIDF_BLEU'].mean(),
    'Luhn': data['Luhn_BLEU'].mean(),
    'LexRank': data['LexRank_BLEU'].mean(),
    'LSA': data['LSA_BLEU'].mean()
}

print("Average ROUGE Scores:")
for method, score in average_rouge.items():
    print(f"{method}: {score:.6f}")

print("\nAverage BLEU Scores:")
for method, score in average_bleu.items():
    print(f"{method}: {score:.6f}")

"""Luhn emerge as the best technique for text summarization. It have highest ROUGE scores with 0.89 and BLEU score with 0.77

# Objective 2: To classify and analyze customer reviews into sentiment categories: positive, negative, and constructive feedback.

## Model Training
"""

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay

"""Use TF-IDF as feature extraction to convert textual data (customer reviews) into numerical vectors for classification tasks"""

# Split data into features and labels
X = data['Cleaned_ReviewText']
y = data['Sentiment']

# Convert text data to TF-IDF features
vectorizer = TfidfVectorizer(max_features=5000)
X_tfidf = vectorizer.fit_transform(X)

# Split dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

# Define models
models = {
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    "Gradient Boosting": GradientBoostingClassifier()
}

# Train and evaluate each model
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"{model_name} Classification Report:\n")
    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")

"""SVM achieve highest accuracy in model training

## Model evaluation
"""

from sklearn.utils.class_weight import compute_class_weight

# Calculate class weights
class_weights = compute_class_weight(class_weight="balanced", classes=np.unique(y_train), y=y_train)
class_weight_dict = dict(zip(np.unique(y_train), class_weights))

# Train a new SVM model with adjusted class weights
svm_model_balanced = SVC(kernel="rbf", class_weight=class_weight_dict)
svm_model_balanced.fit(X_train, y_train)

# Evaluate the new model on the test set
y_pred_balanced = svm_model_balanced.predict(X_test)

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay

# Print classification report
print("Balanced SVM Model Classification Report:\n")
print(classification_report(y_test, y_pred_balanced))

# Print accuracy
accuracy = accuracy_score(y_test, y_pred_balanced)
print(f"Accuracy: {accuracy:.4f}\n")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
# Display the new confusion matrix
cm_balanced = confusion_matrix(y_test, y_pred_balanced, labels=svm_model_balanced.classes_)
disp_balanced = ConfusionMatrixDisplay(confusion_matrix=cm_balanced, display_labels=svm_model_balanced.classes_)
disp_balanced.plot(cmap="Blues", values_format="d")
plt.title("Confusion Matrix for Balanced SVM Model")
plt.show()

# Example of recent reviews
current_reviews = [
    "Love the design but the price is not worth it for this quality. however i still use it daily",
    "This was a gift for a bachelorette. very pretty and sexy without being too over the top. love the print!",
    "Absolutely wonderful - silky and sexy and comfortable",
    "Hangs terribly off the chest. tent-like. looks maternity. no thanks.",
    "Soft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller.",
    "This poncho is so cute i love the plaid check design, the colors look like sorbet & cream and it will pair well with a turtleneck and jeans or pencil skirt and heels. i love this look for fall and it can roll right into spring. great buy!!",
    "These run very small!! they are also short, almost like a crop pant. the fit was so weird that i won't even exchange for a bigger size.",

]

# Predict sentiment for reviews
current_reviews_tfidf = vectorizer.transform(current_reviews)
predictions_balanced = svm_model_balanced.predict(current_reviews_tfidf)

# Display the predictions for the recent reviews
for review, sentiment in zip(current_reviews, predictions_balanced):
    print(f"Review: {review}\nPredicted Sentiment: {sentiment}\n")

import joblib
joblib.dump(svm_model_balanced, "svm_model_balanced.pkl")  # Save the balanced SVM model

from google.colab import files
files.download("svm_model_balanced.pkl")

"""# Objective 3: To predict if a customer will recommend the product based on the textual content of their reviews"""

from sklearn.metrics.pairwise import cosine_similarity

# Use the full dataset for feature extraction
dataset_tfidf = data['Cleaned_ReviewText']
y_data = data['Recommended_IND']  # Assuming Recommended_IND is binary (0 for No, 1 for Yes)

# Convert the full dataset text data to TF-IDF features
X_data_tfidf = vectorizer.transform(dataset_tfidf)

# Split the full dataset into training and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_data_tfidf, y_data, test_size=0.2, random_state=42
)

# Train the SVM model on the full dataset
SVM_model = SVC(kernel="linear", class_weight="balanced", random_state=42)
SVM_model.fit(X_train, y_train)

# Predict on the test set
y_pred = SVM_model.predict(X_test)

# Evaluate the model
print("Classification Report for All Reviews:\n")
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}\n")

# Predict on new reviews
new_reviews = [
    "I ordered this and hoped it would be a fun & whimsical sweater for this winter. instead, it's just strange. it didn't fit well. too short in the overall length & in the sleeves. the shades of the yarn were drab. it also didn't seem made to last. it looked like it would pill/snag in a short amount of time. i returned it. i'd love to see a style similar to this executed better in the future. having a unique sleeve is a cute idea",
    "Soft and pretty, but way too big and wide. love the bell-sleeve detail. i'm exchanging for one size smaller",
    "Nice jeans, but had to return. too tight in hips/thighs and big in waist.",
    "Absolutely wonderful - silky and sexy and comfortable",
    "Beautifully made pants and on trend with the flared crop.so much cuter in person. love these!",
    "I agree with the other reviewers that this runs big. unfortunately there is nothing smaller than an xs so this is going back. i love the lyocell material, i have another skirt with this material and wanted to love this as well but it is too big and with the drawstring pulled tight to fit it doesn't look right. it is not as pictured as well. the skirt i received is a medium blue denim, not the lovely light blue as pictured, another reason why i am returning.",
    "I ordered this in two colors, in my regular size. i washed and laid flat to dry....it shrunk like crazy. i held the one i washed up to the other unworn/unwashed one and it was at least two inches shorter. they may both be going back. i am so sad"
]

new_reviews_tfidf = vectorizer.transform(new_reviews)
recommend_predictions = SVM_model.predict(new_reviews_tfidf)

# Display predictions for new reviews
for review, recommend, tfidf_vec in zip(new_reviews, recommend_predictions, new_reviews_tfidf):
    recommendation = "Recommend" if recommend == 1 else "Do not recommend"
    print(f"Review: {review}\nPredicted Recommendation: {recommendation}\n")

    if recommendation == "Do not recommend":
        # Find the most similar review in the full dataset using cosine similarity
        tfidf_vec = tfidf_vec.reshape(1, -1)  # Reshape for compatibility with cosine_similarity
        similarities = cosine_similarity(tfidf_vec, X_data_tfidf)
        most_similar_idx = np.argmax(similarities)
        category = data.iloc[most_similar_idx]['Class_Name']  # Extract the category

        # Suggest alternative products from the identified category
        alternatives = data[
            (data['Class_Name'] == category) &
            (data['Rating'] >= 4) &
            (data['Recommended_IND'] == 1)
        ].sort_values(by=['P_Feedback_Count'], ascending=False).head(3)  # Top 3 alternatives

        print(f"Category Identified: {category}")
        if not alternatives.empty:
            print(f"Suggested Alternatives for Category: {category}")
            for idx, row in alternatives.iterrows():
                print(f"Product ID: {row['Clothing_ID']}, Rating: {row['Rating']}, Feedback Count: {row['P_Feedback_Count']}")
        else:
            print(f"No suitable alternatives found in category: {category}")
    print("-" * 50)

import joblib

from google.colab import files

# Save the trained SVM model
joblib.dump(SVM_model, "svm_model.pkl")

# Save the TF-IDF vectorizer
joblib.dump(vectorizer, "tfidf_vectorizer.pkl")

# Save the TF-IDF matrix of the dataset
joblib.dump(X_data_tfidf, "dataset_tfidf.pkl")

files.download("svm_model.pkl")
files.download("tfidf_vectorizer.pkl")
files.download("dataset_tfidf.pkl")

data.to_csv("cleaned_reviews.csv", index=False)

files.download("cleaned_reviews.csv")